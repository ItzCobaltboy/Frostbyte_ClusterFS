# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Frostbyte ClusterFS is a distributed, encrypted, chunked object storage system (S3-style) with fully modular microservices. Written in Java 23 + Spring Boot 3.2.5, deployed via Docker. Every file is streamed, split into chunks, encrypted, and distributed across multiple DataNodes with redundancy and zero-trust pipeline.

## Build and Test Commands

This is a Maven multi-module project. All commands should be run from the repository root.

### Build Commands

Building to be done manually by user since mvn is not installed on terminal, we are using IntelliJ's built in

### Test Commandsus

### Running Nodes

Each node packages into a `.jar` with external `config.json` support:
```bash
# After mvn package, JARs are in target/ directories
java -jar frostbyte-datanode/target/frostbyte-datanode-0.1.0.jar
java -jar frostbyte-masternode/target/frostbyte-masternode-0.1.0.jar
java -jar frostbyte-clientnode/target/frostbyte-clientnode-0.1.0.jar
java -jar frostbyte-databaseNode/target/frostbyte-databaseNode-0.1.0.jar
```

## UUID Generation Architecture

**Critical**: All UUIDs are generated by DatabaseNode, NOT by ClientNode or utility classes.

- **sessionId**: Generated by DatabaseNode in `UploadController.initializeUpload()` via `UploadSessionService`
- **fileId**: Generated by DatabaseNode in `UploadController.initializeUpload()` via `FileService`
- **chunkId**: Generated by DatabaseNode in `KeyController.generateKey()` via `KeyService.generateAndStoreKey()`

ClientNode extracts these IDs from responses and uses them as-is. No random UUID generation on client side.

## Client Lifecycle & Data Flow

### Upload Flow (10GB File Example)

1. **User initiates upload** to ClientNode endpoint
   - ClientNode creates an upload session with unique session ID
   - ClientNode queries MasterNode for available Balancer node IP

2. **Stream to Balancer**
   - ClientNode streams file chunks in real-time to BalancerNode
   - Each chunk is split client-side and encrypted individually

3. **Key generation & encryption per chunk**
   - ClientNode requests unique AES-256-GCM encryption key from DatabaseNode (KeyService)
   - ClientNode encrypts each chunk with its unique key
   - Encrypted keys are stored in DatabaseNode (zero-trust model)

4. **Chunk distribution across DataNodes**
   - BalancerNode creates multiple replicas of each chunk (configurable count)
   - BalancerNode queries MasterNode for available DataNodes
   - BalancerNode allocates replicas across DataNodes using Latin rectangle pattern (no two replicas on same node)
   - BalancerNode sends encrypted chunk to selected DataNodes

5. **Persistence**
   - Each DataNode stores chunk as `.snowflake` file (self-contained with metadata + encrypted data)
   - DataNode stores to configured `snowflakeFolder` directory
   - BalancerNode tracks replica placement in DatabaseNode

6. **Session completion**
   - BalancerNode updates upload session status in DatabaseNode
   - ClientNode receives upload confirmation

### Download Flow

1. **User initiates download** with file ID
   - ClientNode queries DatabaseNode for file metadata and chunk locations
   - ClientNode selects random BalancerNode from MasterNode's available nodes

2. **Chunk retrieval from BalancerNode**
   - BalancerNode streams chunks one-by-one from DataNodes (can use any replica)
   - Chunks streamed in order to maintain file integrity

3. **Decryption and streaming**
   - ClientNode receives encrypted chunk from BalancerNode
   - ClientNode retrieves corresponding AES key from DatabaseNode (KeyService)
   - ClientNode decrypts chunk using `ChunkEncryptionService`
   - Decrypted data is added to download stream to user

4. **Download completion**
   - All chunks decrypted and reconstructed
   - File delivered to user as complete entity

## Architecture

### Five-Node Microservice Design

1. **ClientNode** (Public API Gateway)
   - Port: 8080 (default)
   - Receives upload/download requests from users
   - Initiates sessions and streams files to BalancerNode
   - Pings MasterNode for Balancer IP
   - Controllers: `ClientController.java`
   - Key services: `AsyncUploadService`, `SessionManager`, `ChunkEncryptionService`, `KeyClient`

2. **BalancerNode** (Core Logic)
   - Handles upload/download sessions
   - Splits files into chunks
   - Allocates chunks to DataNodes with redundancy using Latin rectangle pattern
   - Coordinates chunk distribution, retry, rollback
   - Manages replica logic

3. **DataNode** (Storage Unit)
   - Port: 6960 (default)
   - Stores chunk binary data as `.snowflake` files
   - Sends periodic heartbeats to MasterNode
   - Rejects uploads if near capacity (507 Insufficient Storage)
   - Controllers: `Datanode_controller.java`
   - Key services: `StartupRunner` (registers with master on boot)
   - Storage folder: `chunks/` (configurable via `config.json` `snowflakeFolder`)

4. **MasterNode** (Coordinator)
   - Port: 7001 (default)
   - Maintains alive node registry via heartbeats
   - Provides ClientNode and BalancerNode with list of alive nodes
   - Detects failure via missed heartbeats
   - Controllers: `Masternode_controller.java`
   - Key services: `heartbeatRegister.java`

5. **DatabaseNode** (Metadata DB)
   - Port: 8082 (default)
   - PostgreSQL backend for metadata storage
   - Controllers: `UploadController`, `ReplicaController`, `KeyController`
   - Key services: `FileService`, `ChunkMetadataService`, `UploadSessionService`, `KeyService`
   - Entities: `File`, `Chunk`, `ChunkReplica`, `UploadSession`, `KeyPair`

### Key Data Structures

**Snowflake** (`frostbyte-clientnode/models/Snowflake.java`):
- Custom serialization format for encrypted chunks
- Structure: `[8-byte metadata length][JSON metadata][encrypted data]`
- Contains: `snowflakeUuid`, `fileUuid`, `chunkNumber`, `totalChunks`, `crcChecksum`, `encryptedData`
- CRC32 checksum for data integrity verification

**Encryption Model**:
- Every chunk encrypted with unique AES-256-GCM key
- Keys generated dynamically per chunk
- AES keys encrypted with master key and stored in DatabaseNode
- Encryption service: `ChunkEncryptionService.java`
- 12-byte IV prepended to encrypted data

**Authentication**:
- All internal node-to-node REST calls authenticated with `X-API-Key` header
- API key stored in `config.json` as `masterAPIKey`
- Same key shared across all nodes (internal trust boundary)

## Configuration

Each node reads `config.json` from `src/main/resources/` via `ConfigLoader` service.

**DataNode config.json**:
```json
{
  "host": "127.0.0.1",
  "port": 6960,
  "nodeName": "Datanode_1",
  "masterNodes": ["127.0.0.1:7000"],
  "snowflakeFolder": "chunks",
  "masterAPIKey": "ABCDEFEG",
  "size": 5
}
```

**MasterNode config.json**:
```json
{
  "host": "127.0.0.1",
  "port": 7001,
  "nodeName": "Masternode_1",
  "masterAPIKey": "ABCDEFEG"
}
```

**DatabaseNode config.json**:
```json
{
  "database": {
    "host": "localhost",
    "port": 5432,
    "name": "frostbyte_metadata_db",
    "username": "postgres_user",
    "password": "your_secure_password"
  },
  "host": "127.0.0.1",
  "port": 8082,
  "nodeName": "DatabaseNode-01",
  "masterNodes": ["127.0.0.1:7000", "127.0.0.1:7001"],
  "masterAPIKey": "ABCDEFEG"
}
```

## Testing Patterns

All modules use Spring Boot Test with MockMvc for controller testing.

Test structure:
- `@SpringBootTest` + `@AutoConfigureMockMvc` for integration tests
- `@ActiveProfiles("test")` for test configuration
- API key authenticated via `X-API-Key` header in tests
- Example: `DatanodeControllerTest.java`, `MasternodeControllerTest.java`, `HeartbeatRegisterTest.java`

Common test scenarios:
- Unauthorized access (invalid API key)
- Empty/missing request data
- Duplicate operations (conflict scenarios)
- Insufficient storage (507 status)
- Not found scenarios (404)

## Development Status

**Completed**:
- DataNode and MasterNode registration and heartbeat system
- DataNode chunk upload/download with storage management
- DatabaseNode with full entity model and services
- Encryption infrastructure (AES-256-GCM with IV)
- Snowflake serialization format with CRC integrity checks
- ClientNode upload session management

**Testing Status**:
- Elaborate unit tests for MasterNode and DataNode completed
- DatabaseNode testing pending

**Pending Implementation**:
- BalancerNode module (chunk allocation logic)
- Latin rectangle redundancy algorithm
- Session rollback mechanism
- Download flow reconstruction
- Orphaned chunk garbage collection

## Important Implementation Notes

- **Zero-trust architecture**: Files encrypted immediately upon entry to system
- **Chunk allocation**: Uses Latin rectangle pattern to ensure no two replicas of same chunk on same node
- **Error handling**: Upload failures trigger rollback to delete stored chunks
- **Session management**: Upload/download sessions tracked in DB with TTL
- **Heartbeat system**: Nodes register on startup (`StartupRunner`) and send periodic heartbeats to MasterNode
- **Storage limits**: DataNodes reject uploads when near capacity (configurable `size` in GB)
- **File format**: `.snowflake` files are self-contained with metadata header + encrypted chunk data

## Local Deployment & Testing

A complete local test environment is available in `test-deployment/` for testing the upload and file reconstruction workflow.

### Quick Start

```bash
# 1. Start PostgreSQL
docker run --name frostbyte-postgres -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=frostbyte_test_db -p 5432:5432 -d postgres:15

# 2. Build all modules (via IntelliJ or Maven)
mvn clean install -DskipTests

# 3. Copy configs
cp test-deployment/*config.json frostbyte-*/

# 4. Start MasterNode (Terminal 1)
cd frostbyte-masternode && java -jar target/frostbyte-masternode-0.1.0.jar

# 5. Start DatabaseNode (Terminal 2)
cd frostbyte-databaseNode && java -jar target/frostbyte-databaseNode-0.1.0.jar

# 6. Start ClientNode (Terminal 3)
cd frostbyte-clientnode && java -jar target/frostbyte-clientnode-0.1.0.jar

# 7. Upload test file
curl -X POST -F "file=@test-file.bin" http://127.0.0.1:8080/api/upload

# 8. Reconstruct file with Python
cd test-deployment
python reconstruct_file.py http://127.0.0.1:8082 <fileId> ./reconstructed.bin --snowflakes ./snowflakes
```

### What Happens During Test

**Upload Lifecycle** (simulated without BalancerNode):

```
User uploads 5MB file
  ↓
ClientNode chunks into 5x 1MB pieces
  ↓
For each chunk:
  - Request chunkId & encrypted AES key from DatabaseNode (/keys/generate)
  - Decrypt key with session private key
  - Encrypt chunk with decrypted AES key (AES-256-GCM)
  - Create .snowflake file with metadata + encrypted data
  - Store in ./test-deployment/snowflakes/
  - Register chunk metadata in DatabaseNode (/upload/chunk/register)
  ↓
Upload completes, sessionId returned to user
```

**File Reconstruction Lifecycle** (Python script):

```
Python script provided with fileId
  ↓
Query DatabaseNode for file metadata (/upload/file/{fileId})
  ↓
Load all .snowflake files from storage folder
  ↓
For each snowflake:
  - Parse metadata and encrypted data
  - Get chunkId from metadata
  - Request encrypted AES key from DatabaseNode (/keys/retrieve)
  - Decrypt AES key (in test, no RSA - would use in production)
  - Decrypt chunk data using AES-256-GCM
  - Write plaintext to output file in order
  ↓
Reconstructed file ready
```

### Test Files

- **masternode-config.json**: MasterNode configuration (7001)
- **databasenode-config.json**: DatabaseNode configuration (8082, PostgreSQL)
- **clientnode-config.json**: ClientNode configuration (8080, stores snowflakes locally)
- **reconstruct_file.py**: Python utility for file reconstruction and validation
- **DEPLOYMENT_GUIDE.md**: Detailed step-by-step deployment walkthrough

### Configuration for Testing

**clientnode-config.json** key setting:
```json
{
  "snowflakeStorageFolder": "./test-deployment/snowflakes"
}
```

When set, AsyncUploadService stores encrypted chunks in this folder instead of sending to BalancerNode.

### Verification

After reconstruction, verify file integrity:
```bash
# Compare binary hashes
md5sum original.bin reconstructed.bin

# Should output identical hashes if reconstruction successful
```